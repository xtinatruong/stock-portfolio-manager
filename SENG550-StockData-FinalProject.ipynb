{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "0kvtPmuZKfru"
   },
   "source": [
    "# SENG 550 Project\n",
    "Authors:\n",
    "- Sydney Kwok (30073206) \n",
    "- Liam Conway (30046856)\n",
    "- Isabella Guimet (30040654)\n",
    "- Christina Truong (30064426)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark is currently setup to run on a local machine. Please change as necessary if you wish to run on a different environment (Colab, Databricks, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "To9reEGekk-T"
   },
   "outputs": [],
   "source": [
    "%pip install pyspark\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#Create SparkSession\n",
    "spark = SparkSession.builder.master(\"local[8]\").appName(\"Stock_Data\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "15-xw65skr_H"
   },
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "DYYUbDXjjHtx"
   },
   "source": [
    "# Creating Our Dataset \n",
    "Use the yfinance library to fetch historical stock data from Yahoo Finance for all stocks in the S&P 500 over the last 12 years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "28e7YXdbj5qi",
    "outputId": "cf77d6ca-604b-4d1b-fd76-d5f7baad3c97"
   },
   "outputs": [],
   "source": [
    "!pip install yfinance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "be9oB12VjNaw",
    "outputId": "d33f40ad-c844-4818-e856-62c441d833ef"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def get_sp500():\n",
    "    # Get S&P 500 data from Wikipedia\n",
    "    data = pd.read_html('http://en.wikipedia.org/wiki/List_of_S%26P_500_companies')[0]\n",
    "    # Take only the selected columns\n",
    "    data = data[['Symbol', 'Security', 'GICS Sector']]\n",
    "    # Rename the selected columns\n",
    "    data = data.rename(columns={'Symbol': 'Ticker', 'Security': 'Company', 'GICS Sector': 'Sector'})\n",
    "    # Replace '.' with '-' for tickers (required for Yahoo Finance)\n",
    "    data['Ticker'] = data['Ticker'].str.replace('.', '-', regex=True)\n",
    "    # Remove commas from company names to prevent CSV misinterpretation\n",
    "    data['Company'] = data['Company'].str.replace(',', '', regex=True)\n",
    "    return data\n",
    "\n",
    "companies = get_sp500()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download all financial data for the S&P 500 tickers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "\n",
    "finance_data = yf.download(tickers=companies['Ticker'].tolist(), start='2010-01-01', end='2022-11-22', group_by='ticker')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge company data and financial data from the last two steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = []\n",
    "for index, company_data in companies.iterrows():\n",
    "    merged_data = finance_data[company_data['Ticker']].assign(**company_data)\n",
    "    dataframes.append(merged_data)\n",
    "\n",
    "final_df = pd.concat(dataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O52_wd-7aOXt",
    "outputId": "b72aa789-611d-40b0-c75e-bd59cc5030ab"
   },
   "outputs": [],
   "source": [
    "final_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 237
    },
    "id": "c63ZPsylTnlc",
    "outputId": "e55b81a9-410d-4a6a-8ac2-faa28fcddd40"
   },
   "outputs": [],
   "source": [
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 237
    },
    "id": "GLV3zKCbToNk",
    "outputId": "60be5313-ba6f-4428-eb23-78eae2eaec09"
   },
   "outputs": [],
   "source": [
    "final_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BDYPpsKKbWIc",
    "outputId": "5db2bffd-e7d6-4c11-c9b2-33e16c203545"
   },
   "outputs": [],
   "source": [
    "final_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wxbE1ragV2YX",
    "outputId": "927b6ad1-e025-4c40-a497-9ece5a2f879a"
   },
   "outputs": [],
   "source": [
    "final_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The null data consists of stock financial data that did not exist as of our given start date (January 1, 2010), as seen below. For instance, ZTS was listed on February 1, 2013."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_df[final_df.isnull().any(axis=1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As these stocks did not have any *public* value as of these dates, we'll fill all NaN values of 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = final_df.fillna(0)\n",
    "final_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZU3BSoB1jw9y"
   },
   "outputs": [],
   "source": [
    "final_df.to_csv('raw-data.csv', header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DA3dJCFnponl"
   },
   "source": [
    "# Load CSV Into PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RGnt_eoqup1a"
   },
   "source": [
    "Read the CSV and return it as an RDD of Strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mYT-twMpwonR"
   },
   "outputs": [],
   "source": [
    "rdd = sc.textFile('raw-data.csv', 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zmLxLHLyxVPT",
    "outputId": "5a8db782-ae27-4541-a82b-18433e459697"
   },
   "outputs": [],
   "source": [
    "rdd.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8KgBNvjPpuLs"
   },
   "source": [
    "# Exploratory Data Analysis\n",
    "Compute some stats on the historical stock data we've collected for the selected ten companies for the period January 1, 2010 to November 22, 2022."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SYtEsVe3dQSv"
   },
   "source": [
    "**1). Calculate the average close price for each ticker over the last 20 years**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fWUh-TbqfTsb"
   },
   "source": [
    "1a). Extract fields only relevant to this analysis. That is, we only need field 0 (Date), field 4 (Close) and field 7 (ticker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xdFScpknhLya",
    "outputId": "c0b25ccf-fc08-438e-ece8-d6d0cefd9971"
   },
   "outputs": [],
   "source": [
    "def extractFieldsForQ1(stockRDDRecord):\n",
    "  fieldsList = stockRDDRecord.split(\",\")\n",
    "  return (fieldsList[0], fieldsList[4], fieldsList[7])\n",
    "\n",
    "print(extractFieldsForQ1(rdd.take(1)[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "btH83OmriEek"
   },
   "source": [
    "Now, we apply this function \"extractFieldsForQ1()\" for all rows in our RDD using a map function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AmOF-WyViOf2",
    "outputId": "0d3e30d4-8bd4-46e9-d10e-de9046f224d6"
   },
   "outputs": [],
   "source": [
    "closeTickerRDD = rdd.map(extractFieldsForQ1)\n",
    "print(closeTickerRDD.take(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "closeTickerRDDFiltered = closeTickerRDD.filter(lambda x: float(x[1]) > 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BMF7QUgleAeB"
   },
   "source": [
    "1b). Calculate the number of close stock prices for each ticker. From above, we can see that the `closeTickerRDD` contains 3 values: ('date', 'close', 'ticker'), so we need to grab the 2nd index which is ticker and count how many rows we have for that ticker. We will save this value for later when we do the average calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r0tDwXATeXt6",
    "outputId": "5eeab2a2-47fe-4785-99b0-bb6e63149eff"
   },
   "outputs": [],
   "source": [
    "stockCountPerTickerRDD = (closeTickerRDDFiltered.map(lambda x : (x[2], 1))\n",
    "                            .reduceByKey(lambda x,y : x+y)\n",
    "                            .take(10)\n",
    "                      )\n",
    "print(stockCountPerTickerRDD)\n",
    "\n",
    "# Will help out later when calculating the averages\n",
    "stockCountPerTickerDict = dict(stockCountPerTickerRDD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UzxYsECopWo8"
   },
   "source": [
    "1c). Calculate the sum of all close stock prices for a particular stock for the last 20 years. Reminder that closeTickerRDD contains 3 values: ('date', 'close', 'ticker'). Ticker is `x[2]` and close is `x[1]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jFzXRHtkps0V",
    "outputId": "694d5c4a-f35f-49ad-f474-eea045a400c6"
   },
   "outputs": [],
   "source": [
    "sumClosePricePerTickerRDD = (closeTickerRDDFiltered.map(lambda x : (x[2], float(x[1])))\n",
    "                                .reduceByKey(lambda x,y: x+y)\n",
    "                                .take(10))\n",
    "print(sumClosePricePerTickerRDD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Fr81M02vA6M"
   },
   "source": [
    "1d) Calculate the average stock close price with `stockCountPerTickerRDD` and `avgClosePricePerTickerRDD`. \n",
    "\n",
    "Note:\n",
    "\n",
    "item[1] is the sum of close price per ticker\n",
    "\n",
    "stockCountPerTickerDict.get(item[0]) is the total number of stocks close prices for that ticker in `item[0]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O_J-2e2PpWOe",
    "outputId": "3811a7b7-b5f8-47c1-9df6-59c89080fa7a"
   },
   "outputs": [],
   "source": [
    "# This will hold the average stock close prices for each ticker\n",
    "avgStockClosePrices = list()\n",
    "\n",
    "for item in sumClosePricePerTickerRDD:\n",
    "  avg = item[1]/stockCountPerTickerDict.get(item[0])\n",
    "  avgStockClosePrices.append((item[0], avg))\n",
    "\n",
    "sortedAvgStock = sorted(avgStockClosePrices, key=lambda x: x[1], reverse=True);\n",
    "print(sortedAvgStock)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jju7CrazBI24"
   },
   "source": [
    "**2). Calculate the min \"Low\" value in this dataset for each ticker**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vtc01ULCB9PB",
    "outputId": "4ef1f555-fff6-434c-ff68-60f8d309dc6a"
   },
   "outputs": [],
   "source": [
    "# Just extract Ticker(7) & Low(3)\n",
    "def extractFieldsForQ2(stockRDDRecord):\n",
    "  fieldsList = stockRDDRecord.split(\",\")\n",
    "  return (fieldsList[7], float(fieldsList[3]))\n",
    "\n",
    "print(extractFieldsForQ2(rdd.take(1)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WrsqfxCsDKS-",
    "outputId": "1baa2f50-e95a-4dab-81ec-12b087f82b6c"
   },
   "outputs": [],
   "source": [
    "# Apply extractFieldsForQ2 on all rows in the RDD\n",
    "low_rdd = rdd.map(extractFieldsForQ2)\n",
    "print(low_rdd.take(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U0H94AwUENhP",
    "outputId": "fb605e44-fcc6-4ec3-a6c7-3b9f1b33a4b0"
   },
   "outputs": [],
   "source": [
    "# Calculate the min Low value for each ticker\n",
    "min_low_by_ticker = low_rdd.filter(lambda x: x[1] > 0).reduceByKey(lambda x,y: min(x,y)).take(10)\n",
    "sorted(min_low_by_ticker, key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ytS_xppvA6He"
   },
   "source": [
    "**3). Calculate the max \"High\" value in this dataset for each ticker**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1OkaKzt7CAkq",
    "outputId": "8b1c1780-7112-4038-cfd6-8008c5f18a89"
   },
   "outputs": [],
   "source": [
    "# Just extract Ticker(7) & High(2)\n",
    "def extractFieldsForQ3(stockRDDRecord):\n",
    "  fieldsList = stockRDDRecord.split(\",\")\n",
    "  return (fieldsList[7], float(fieldsList[2]))\n",
    "\n",
    "print(extractFieldsForQ3(rdd.take(1)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W2xCClQMDRdN",
    "outputId": "cc941894-1717-4fc0-c98d-79f32f12928a"
   },
   "outputs": [],
   "source": [
    "# Apply extractFieldsForQ3 on all rows in the RDD\n",
    "high_rdd = rdd.map(extractFieldsForQ3)\n",
    "print(high_rdd.take(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N2DKW9hdFbmW",
    "outputId": "3d69b172-c80b-4772-e2a8-5aed0f57bf8f"
   },
   "outputs": [],
   "source": [
    "# Calculate the max High value for each ticker\n",
    "max_high_by_ticker = high_rdd.reduceByKey(lambda x,y: max(x,y)).take(10)\n",
    "sorted(max_high_by_ticker, key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "m7GhwJcxSwJB"
   },
   "source": [
    "**4. Graph the max \"High\" value in this dataset for each ticker per year**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0aDe8gqLULE4"
   },
   "source": [
    "4a). Extract fields only relevant to this analysis. That is, we only need field 0 (Date), 2 (High), and 7 (Ticker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ndRKikYtTNrb",
    "outputId": "6f205f8c-9e23-4c4e-f538-6014c6a431f2"
   },
   "outputs": [],
   "source": [
    "def extractFieldsForQ4(stockRDDRecord):\n",
    "  fieldsList = stockRDDRecord.split(\",\")\n",
    "  return (fieldsList[0], fieldsList[2], fieldsList[7]) # Extract Date (0), High (2), Ticker (7)\n",
    "\n",
    "print(extractFieldsForQ4(rdd.take(1)[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qKlIcFH3US8o"
   },
   "source": [
    "Now, we apply this function \"extractFieldsForQ4()\" for all rows in our RDD using a map function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3c9ubkWmTQe1",
    "outputId": "9d587994-7a7a-49cf-80d5-2b7ee156a172"
   },
   "outputs": [],
   "source": [
    "graphHighRDD = rdd.map(extractFieldsForQ4)\n",
    "print(graphHighRDD.take(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ReNcDIVUff_"
   },
   "source": [
    "4b). Calculate the high price of each year for each ticker. From above, we can see that the `graphHighRDD` contains 3 values: `('date', 'high', 'ticker')`. In order to parse the data properly, we'll need to extract only the year out of the date, group all the data by the ticker, group the highs by year, and then take the max of the highs. An example array element may look like the following:\n",
    "```python\n",
    "('AAAA', {2000: 1, 2001: 2})\n",
    "```\n",
    "\n",
    "We will save this value for later when drawing the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rlYD6PTDTU2G",
    "outputId": "ceca30f8-eef6-4fe8-c1c9-8f4a1dbd13cc"
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Function to group the highs by year, and place the maximum high into a dict\n",
    "def merge(x):\n",
    "    data = list(map(lambda y : (y[1], y[2]), x[1]))\n",
    "    years = {y[0] for y in data}\n",
    "    result = defaultdict(int)\n",
    "    for d in data:\n",
    "        year, value = int(d[0]), float(d[1])\n",
    "        if(value > result[year]):\n",
    "            result[year] = value\n",
    "\n",
    "    return x[0], dict(result)\n",
    "\n",
    "# Convert the data into the \n",
    "graphHighPerYearRDD = (graphHighRDD.map(lambda x : (x[2], x[0].split('-')[0], x[1]))\n",
    "                            .groupBy(lambda x : x[0])\n",
    "                            .map(merge))\n",
    "\n",
    "print(graphHighPerYearRDD.take(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qVDqhfTvVLJo"
   },
   "source": [
    "4c) Draw the graph from the `graphHighPerYearRDD` dataset above, using matplotlib.\n",
    "\n",
    "As some stocks may not have been around for the entire duration, we'll need to determine every unique year contained within all the stocks. If a stock does not contain data for a year, we'll assume the price is 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "e-kSVtZVTxPR",
    "outputId": "e348867c-1e15-42a2-849c-8ee76f7d9947"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "graphHighValues = graphHighPerYearRDD.take(10)\n",
    "\n",
    "# Determine every unique year contained within the graphHighPerYearRDD\n",
    "x_values = graphHighPerYearRDD.flatMap(lambda x: list(x[1].keys())).distinct().collect()\n",
    "x_values.sort()\n",
    "\n",
    "# Display the output for each unique ticker\n",
    "for row in graphHighValues:\n",
    "    y_values = []\n",
    "    for key in x_values:\n",
    "        y_values.append(row[1][key] if key in row[1] else 0)\n",
    "    \n",
    "    plt.plot(x_values, y_values, label=row[0])\n",
    "\n",
    "# Stylize and display the graph\n",
    "plt.title(\"Yearly Highs by Ticker\")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"High\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. What is the most active month for trading volumes?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractFieldsForQ5(stockRDDRecord):\n",
    "  fieldsList = stockRDDRecord.split(\",\")\n",
    "  return (fieldsList[0], float(fieldsList[6]))\n",
    "\n",
    "print(extractFieldsForQ5(rdd.take(1)[0]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "btH83OmriEek"
   },
   "source": [
    "Now, we apply this function `extractFieldsForQ5()` for all rows in our RDD using a map function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volumeRDD = rdd.map(extractFieldsForQ5)\n",
    "print(volumeRDD.take(5))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "BMF7QUgleAeB"
   },
   "source": [
    "5b). Calculate the total volume generated by date. This sums up all the individual volumes by each stock, and gives us the volume on each unqiue date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalVolumePerDateRDD = volumeRDD.map(lambda x: (x[0], (x[1]))).reduceByKey(lambda a,b: a+b)\n",
    "print(totalVolumePerDateRDD.take(5))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5c). Calculate the total volume per month, and total number of days read from each month. This will allow us to average out the results later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalVolumeByMonthRDD = totalVolumePerDateRDD.map(lambda x: (int(x[0].split('-')[1]), (x[1], 1))).reduceByKey(lambda x,y: (x[0]+y[0], x[1]+y[1]))\n",
    "print(totalVolumeByMonthRDD.collect())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5d). Given the total volume per month and the total number of days, find the average volume by month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avgVolumeByMonthRDD = totalVolumeByMonthRDD.map(lambda x: (x[0], (x[1][0] / x[1][1])))\n",
    "avgVolumeByMonth = avgVolumeByMonthRDD.collect()\n",
    "print(avgVolumeByMonth)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5e). Print out the average volume per month, with the largest volume printed first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import calendar\n",
    "\n",
    "sortedAvgVolumeByMonth = sorted(avgVolumeByMonth, key=lambda x: x[1], reverse=True)\n",
    "print(\"--- Average Volume per Month ----\")\n",
    "for month_num, volume in sortedAvgVolumeByMonth:\n",
    "    print(f\"{calendar.month_name[month_num]}: {volume}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. What is the day of the week with the highest returns?**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6a). Extract fields only relevant to this analysis. That is, we only need field 0 (Date), field 1 (Open) and field 6 (Volume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractFieldsForQ6(stockRDDRecord):\n",
    "  fieldsList = stockRDDRecord.split(\",\")\n",
    "  return (fieldsList[0], float(fieldsList[1]), float(fieldsList[4])) # Date (0), Open (1), Close (4)\n",
    "\n",
    "print(extractFieldsForQ6(rdd.take(1)[0]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we apply this function `extractFieldsForQ6()` for all rows in our RDD using a map function. We'll also filter out any columns with a non-positive open or close. This is to prevent us from seeing stocks that are not listed yet at the given date (i.e. open and close are 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dailyReturnRDD = rdd.map(extractFieldsForQ6).filter(lambda x: x[1] > 0 and x[2] > 0)\n",
    "print(dailyReturnRDD.take(5))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6b). Convert the open and close for each date to the earnings percentage, and add the count to determine the number of stocks & dates for averaging later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertToEarnings(x):\n",
    "    return (x[2] - x[1]) / x[1] * 100\n",
    "\n",
    "\n",
    "earningsRDD = dailyReturnRDD.map(lambda x: (x[0], (convertToEarnings(x), 1))).reduceByKey(lambda x,y: (x[0]+y[0], x[1]+y[1]))\n",
    "print(earningsRDD.take(5))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6c). Get the total earnings per weekday, and the collective count. The weekday is mapped between 0-4 (where 0 is Monday, 4 is Friday)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "dayEarningsRDD = earningsRDD.map(lambda x: (datetime.datetime.strptime(x[0], '%Y-%m-%d').weekday(), x[1])).reduceByKey(lambda x,y: (x[0]+y[0], x[1]+y[1]))\n",
    "print(dayEarningsRDD.take(5))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6d). Determine the average earnings per weekday, using the total earnings and the collective count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalEarningsRDD = dayEarningsRDD.map(lambda x: (x[0], x[1][0] / x[1][1]))\n",
    "finalEarnings = finalEarningsRDD.collect()\n",
    "print(finalEarnings)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6e). Print out the average earnings per weekday, with the largest earnings printed first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sortedFinalEarnings = sorted(finalEarnings, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"--- Average Earnings per Weekday ---\")\n",
    "for weekday_num, earnings in sortedFinalEarnings:\n",
    "    print(f\"{calendar.day_name[weekday_num]}: {earnings}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "92YvE5MXpw6s"
   },
   "source": [
    "# Developing & Testing ML Models\n",
    "Following \"Spark Tutorial 2 - ML\", we complete a supervised learning pipeline, using the Stock Data dataset we looked at in our EDA phase of the project. Our goal is to train a linear regression model to predict Close values given the Date, Ticker, Open, High, Low, & Volume.\n",
    "\n",
    "## Read & Parse The Initial Dataset\n",
    "We have already previously completed the process of reading the stock data into an RDD, where each element of the RDD is a comma-separated string containing the Date, Open, High, Low, Close, Adj Close, Volume, and Ticker (representing the stock data for a given stock on a given date).\n",
    "\n",
    "Let's now use the `count` method to see how many data points are in the RDD & the `take` method to preview the first 3 data points in its current raw, comma-separated string format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CALGEdrcp0GS",
    "outputId": "262db0c7-5408-43db-9641-a03ef41db940"
   },
   "outputs": [],
   "source": [
    "num_points = rdd.count()\n",
    "print(f\"There are {num_points} data points in the RDD.\")\n",
    "\n",
    "first_3_points = rdd.take(3)\n",
    "print(first_3_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Po0sC_NB_-b"
   },
   "source": [
    "## RDD Pre-Processing\n",
    "Prior to training our regression model, we must first complete some pre-processing on the RDD. \n",
    "\n",
    "As mentioned previously, the RDD currently contains Date (field 0), Open (1), High (2), Low (3), Close (4), Adj Close (5), Volume (6), and Ticker (7) data, but our goal is to train a linear regression model to predict Close values given the Date, Ticker, Open, High, Low, & Volume. Hence, the Adj Close field (field 5) will not be used in our model, and can now be processed out of our RDD.\n",
    "\n",
    "While we're here, we will also be reformatting Dates from \"YYYY-MM-DD\" to \"YYYY\", \"MM\", \"DD\".\n",
    "\n",
    "Also, we will convert our categorical Ticker data into a numerical format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aLNzrc3uEysd",
    "outputId": "cd315e11-b461-4591-b0b2-11c2104af700"
   },
   "outputs": [],
   "source": [
    "tickers = companies['Ticker'].values.tolist()\n",
    "ticker_classes = {k: v for v, k in enumerate(tickers)}\n",
    "\n",
    "def category_to_num(category):\n",
    "  \"\"\"\n",
    "  Given a Ticker name/category, return the corresponding numerical value.\n",
    "  \"\"\"\n",
    "  return str(ticker_classes[category])\n",
    "\n",
    "def process_record(rdd_record):\n",
    "  \"\"\"\n",
    "  Takes a record of the form 'YYYY-MM-DD, Open, High, Low, Close, Adj Close, Volume, Ticker' and returns a record of the form 'Ticker, YYYY, MM, DD, Open, High, Low, Volume, Close'\n",
    "  \"\"\"\n",
    "  date, open, high, low, close, _, volume, ticker, _, _ = rdd_record.split(',')\n",
    "\n",
    "  year, month, day = date.split('-')\n",
    "\n",
    "  fields = [category_to_num(ticker), year, month, day, open, high, low, volume, close]\n",
    "  return ','.join(fields)\n",
    "\n",
    "processed_rdd = rdd.map(process_record)\n",
    "print(processed_rdd.take(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zH59eASWQAi5"
   },
   "source": [
    "Now, we can use MLlib `LabeledPoint` objects to label our records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PFLHop4DQQcF",
    "outputId": "90e6fa51-d820-4a34-9e17-d0e83c6e7716"
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "import numpy as np\n",
    "\n",
    "# sample data point: '0,2010,01,04,6.8125,6.83050012588501,6.6570000648498535,151998000,6.695000171661377'\n",
    "# we want the Close field to be the label (the last field), and the rest to be features\n",
    "\n",
    "def label_record(record):\n",
    "    \"\"\"\n",
    "    Converts a record from a comma-separated string of the form 'Ticker, YYYY, MM, DD, Open, High, Low, Volume, Close' \n",
    "    into a `LabeledPoint` where Close is the label and the rest of the fields are all features.\n",
    "    \"\"\"\n",
    "    record_elements = record.split(',')\n",
    "    return LabeledPoint(record_elements[-1], record_elements[:-1])\n",
    "\n",
    "labeled_points = processed_rdd.map(label_record)\n",
    "first_point = labeled_points.take(1)\n",
    "first_point_features = first_point[0].features \n",
    "first_point_label = first_point[0].label\n",
    "print(first_point_features, first_point_label)\n",
    "\n",
    "num_features = len(first_point_features)\n",
    "print(f\"There are {num_features} features.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Je3YQmx1ykt8"
   },
   "source": [
    "Now, in order to normalize our values, we're going to scale our features. The result of this is that all of our features will span a similar range.\n",
    "\n",
    "We will follow the same process as shown in the ML_Linear_Regression tutorial: `(featureValue - meanOfFeatureValues) / standardDeviationOfFeatureValues`. \"For a given feature, the mean and standard deviation of its feature values are calculated. Then, the mean is subtracted from each value and the result is then divided by the standard deviation.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mOjFkq7Xz07K",
    "outputId": "499e76ae-6f14-4f03-da9c-47ea9705f0b4"
   },
   "outputs": [],
   "source": [
    "def normalize_features(labeled_point):\n",
    "  \"\"\"\n",
    "  Normalize the features of the LabeledPoint object, labeled_point.\n",
    "  \"\"\"\n",
    "  normalized_features = list()\n",
    "  for i in range(0, len(labeled_point.features)):\n",
    "    feature = (labeled_point.features[i] - broadcast_mean.value[i]) / broadcast_stdev.value[i]\n",
    "    normalized_features.insert(i, feature)\n",
    "  return LabeledPoint(labeled_point.label, normalized_features)\n",
    "\n",
    "\n",
    "def get_normalized_rdd(non_normalized_rdd): \n",
    "    \"\"\"\n",
    "    Normalizes the features of the LabeldPoints contained in non_normalized_rdd.\n",
    "    \"\"\"\n",
    "    mean_list = list()\n",
    "    stdev_list = list()\n",
    "    num_features = len(non_normalized_rdd.take(1)[0].features)\n",
    "    for i in range(0, num_features):\n",
    "        feature_rdd = non_normalized_rdd.map(lambda lp: lp.features[i])\n",
    "        feature_mean = feature_rdd.mean()\n",
    "        feature_stdev = feature_rdd.stdev()\n",
    "        mean_list.insert(i, feature_mean)\n",
    "        stdev_list.insert(i, feature_stdev)\n",
    "    global broadcast_mean \n",
    "    broadcast_mean = sc.broadcast(mean_list)\n",
    "    global broadcast_stdev \n",
    "    broadcast_stdev = sc.broadcast(stdev_list)\n",
    "    return non_normalized_rdd.map(normalize_features)\n",
    "\n",
    "normalized_points = get_normalized_rdd(labeled_points)\n",
    "print(normalized_points.take(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rSDxi35P34dy"
   },
   "source": [
    "Now, we will split our data into training, validation, and testing sets using the `randomSplit` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1XWKIeoZ4MsO",
    "outputId": "2366a1f5-042c-441e-c3ce-10beb91e26e2"
   },
   "outputs": [],
   "source": [
    "weights = [.80, .15, .05] # 80/15/5 train/validate/test split\n",
    "seed = 42\n",
    "\n",
    "train_data, val_data, test_data = normalized_points.randomSplit(weights, seed)\n",
    "\n",
    "train_data.cache()\n",
    "val_data.cache()\n",
    "test_data.cache()\n",
    "\n",
    "num_train = train_data.count()\n",
    "num_val = val_data.count()\n",
    "num_test = test_data.count()\n",
    "\n",
    "print(f\"The original dataset has {normalized_points.count()} data points.\")\n",
    "print(f\"There are {num_train} training data points, {num_val} validation data points, and {num_test} test data points, for a total of {num_train + num_val + num_test} data points.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pfmzO-Fa7rTO"
   },
   "source": [
    "## Creating & Evaluating A Baseline Model\n",
    "Creating a baseline model that we can use to evaluate our machine learning model later. We will look back and see if our model performs better or worse than this baseline model that uses a very simple technique to predict the stock Close values."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "lxmYaDBN8tIN"
   },
   "source": [
    "### Average Label\n",
    "This baseline model always makes the same prediction: the average label in the training set. This is a constant prediction value that is completely independent of the given data point. \n",
    "\n",
    "For this model, we will compute this value: the average Close price for the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x3damlke9zZg",
    "outputId": "5cf316e5-1329-44f5-cc5a-bdd72b1cffa2"
   },
   "outputs": [],
   "source": [
    "avg_close_price = (train_data.map(lambda s: s.label)).mean()\n",
    "print(f\"The average Close price for the training set is {avg_close_price}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_0K8IPb3-aMC"
   },
   "source": [
    "Now, we use Root Mean Squared Error (RMSE) to evaluate how well this baseline model performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HfEbfrUZ-jQV",
    "outputId": "c5eec842-4cc9-4c00-e1a9-375bc17b24c3"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def squared_error(label, prediction):\n",
    "    \"\"\"\n",
    "    Calculates the squared error for a single prediction.\n",
    "    \"\"\"\n",
    "    sqr_error = (label - prediction) * (label - prediction)\n",
    "    return sqr_error\n",
    "\n",
    "def calc_rmse(labels_and_preds):\n",
    "    \"\"\"\n",
    "    Calculates the RMSE for an `RDD` of (label, prediction) tuples.\n",
    "    \"\"\"\n",
    "    sqr_sum = labels_and_preds.map(lambda s: squared_error(s[0], s[1])).sum()\n",
    "    return math.sqrt(sqr_sum / labels_and_preds.count())\n",
    "\n",
    "labels_and_preds_train = train_data.map(lambda s: (s.label, avg_close_price))\n",
    "rmse_train_base = calc_rmse(labels_and_preds_train)\n",
    "print(f\"The RMSE of the baseline model on the train set is {rmse_train_base}.\")\n",
    "\n",
    "labels_and_preds_test = test_data.map(lambda s: (s.label, avg_close_price))\n",
    "rmse_test_base = calc_rmse(labels_and_preds_test)\n",
    "print(f\"The RMSE of the baseline model on the test set is {rmse_test_base}.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Cltxj6OuBHow"
   },
   "source": [
    "## Training Linear Regression Models Using MLlib\n",
    "Now, let's train some machine learning models and see if we can get better performance.\n",
    "### Linear Regression With SGD\n",
    "We're going to start by using LinearRegressionWithSGD to train a model with L2 regularization and an intercept. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b-JsdtHrDhcV",
    "outputId": "88cc21fb-74cb-45a3-8c21-fe45166ead46"
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import DenseVector\n",
    "from pyspark.mllib.regression import LinearRegressionWithSGD\n",
    "# Values to use when training the linear regression model\n",
    "numIters = 500  # iterations\n",
    "alpha = 1.0  # step - learning rate (rate at which you change your weights in each iteration)\n",
    "miniBatchFrac = 1.0  # miniBatchFraction\n",
    "reg = 1e-5  # regParam\n",
    "regType = 'l2'  # regType\n",
    "useIntercept = True  # intercept\n",
    "\n",
    "# train the model\n",
    "firstModel = LinearRegressionWithSGD.train(train_data, numIters, alpha, miniBatchFrac, initialWeights=None, regParam=reg, regType=regType, intercept=useIntercept)\n",
    "\n",
    "# weightsLR1 stores the model weights; interceptLR1 stores the model intercept\n",
    "weightsLR1 = firstModel.weights\n",
    "interceptLR1 = firstModel.intercept\n",
    "\n",
    "print(f\"Linear Regression Model 1 weights: {weightsLR1}, intercept: {interceptLR1}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pln5C8B7FJIF"
   },
   "source": [
    "Now, use the `LinearRegressionModel.predict()` method to make a prediction on a sample point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5tp-eJrkFPt2",
    "outputId": "3532e2bd-e3db-4bc4-8f8d-101f4c42af1c"
   },
   "outputs": [],
   "source": [
    "sample_point = train_data.take(1)[0]\n",
    "sample_prediction = firstModel.predict(sample_point.features)\n",
    "print(sample_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels_and_preds = train_data.map(lambda lp: (lp.label, firstModel.predict(lp.features)))\n",
    "rmse_trainLR1 = calc_rmse(train_labels_and_preds)\n",
    "\n",
    "print(f\"Baseline model train RMSE: {rmse_train_base}.\")\n",
    "print(f\"Linear Regression model train RMSE: {rmse_trainLR1}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7N2PStShF_LM"
   },
   "source": [
    "Now, let's evaluate RMSE of this model on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jkM7HcApF9ze",
    "outputId": "26277573-980b-4a3b-ede7-6e2baf843c8d"
   },
   "outputs": [],
   "source": [
    "labels_and_preds = test_data.map(lambda lp: (lp.label, firstModel.predict(lp.features)))\n",
    "rmse_testLR1 = calc_rmse(labels_and_preds)\n",
    "\n",
    "print(f\"Baseline model test RMSE: {rmse_test_base}.\")\n",
    "print(f\"Linear Regression model test RMSE: {rmse_testLR1}.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "idV7ZJgDG8-m"
   },
   "source": [
    "While we do see great improvement in performance from our baseline model, there's still quite a bit of improvement to be made. We will try different models and hyperparameters to see if we can improve the performance."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DzIG8ztRLBjh",
    "outputId": "3564103d-912c-4cdc-88ed-ac18b8dc0eb1"
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.tree import RandomForest\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "dtModel = RandomForest.trainRegressor(train_data, categoricalFeaturesInfo={},\n",
    "                                      numTrees=16, featureSubsetStrategy=\"auto\",\n",
    "                                      impurity='variance', maxDepth=10, maxBins=128)\n",
    "\n",
    "train_predictions = dtModel.predict(train_data.map(lambda x: x.features)).collect()\n",
    "train_labels = train_data.map(lambda x: x.label).collect()\n",
    "rmseTrainDT = np.sqrt(np.mean((np.array(train_predictions)-np.array(train_labels))**2))\n",
    "\n",
    "test_predictions = dtModel.predict(test_data.map(lambda x: x.features)).collect()\n",
    "test_labels = test_data.map(lambda x: x.label).collect()\n",
    "rmseTestDT = np.sqrt(np.mean((np.array(test_predictions)-np.array(test_labels))**2))\n",
    "\n",
    "print(f\"Baseline model train RMSE: {rmse_train_base}\")\n",
    "print(f\"Random Forest Model train RMSE: {rmseTrainDT}.\")\n",
    "\n",
    "print(f\"Baseline model test RMSE: {rmse_test_base}\")\n",
    "print(f\"Random Forest Model test RMSE: {rmseTestDT}.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "G5UIj4YyMAHl"
   },
   "source": [
    "Still, after hyperparameter tuning, our best performing model is the initial Linear Regression Model. We will now run this model on the test data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.colors import ListedColormap, Normalize\n",
    "from matplotlib.cm import get_cmap\n",
    "\n",
    "cmap = get_cmap('YlOrRd')\n",
    "\n",
    "def preparePlot(xticks, yticks, figsize=(10.5, 10.5), hideLabels=False, gridColor='#999999',\n",
    "                gridWidth=1.0):\n",
    "    \"\"\"Template for generating the plot layout.\"\"\"\n",
    "    plt.close()\n",
    "    fig, ax = plt.subplots(figsize=figsize, facecolor='white', edgecolor='white')\n",
    "    ax.axes.tick_params(labelcolor='#999999', labelsize='10')\n",
    "    for axis, ticks in [(ax.get_xaxis(), xticks), (ax.get_yaxis(), yticks)]:\n",
    "        axis.set_ticks_position('none')\n",
    "        axis.set_ticks(ticks)\n",
    "        axis.label.set_color('#999999')\n",
    "        if hideLabels: axis.set_ticklabels([])\n",
    "    plt.grid(color=gridColor, linewidth=gridWidth, linestyle='-')\n",
    "    map(lambda position: ax.spines[position].set_visible(False), ['bottom', 'top', 'left', 'right'])\n",
    "    return fig, ax"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will display a color-coded scatter plot that compares the actual stock closing prices to those predicted by our best model (Linear Regression with SGD). Lighter colour predictions represent lower errors while the red ones represent large errors. While there are a few outlier values, the graph mostly appears to show the a low error value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.asarray(test_data\n",
    "                         .map(lambda lp: firstModel.predict(lp.features))\n",
    "                         .collect())\n",
    "\n",
    "actual = np.asarray(test_data\n",
    "                    .map(lambda lp: lp.label).collect())\n",
    "\n",
    "error = np.asarray(test_data\n",
    "                   .map(lambda lp: (lp.label, firstModel.predict(lp.features))).map(lambda lp: pow(lp[0] - lp[1], 2))\n",
    "                   .collect())\n",
    "\n",
    "norm = Normalize()\n",
    "clrs = cmap(np.asarray(norm(error)))[:,0:3]\n",
    "\n",
    "fig, ax = preparePlot(np.arange(0, 5000, 500), np.arange(0, 5000, 500))\n",
    "ax.set_xlim(0, 5000), ax.set_ylim(20, 5000)\n",
    "plt.scatter(predictions, actual, s=14**2, c=clrs, edgecolors='#888888', alpha=0.75, linewidths=.5)\n",
    "ax.set_xlabel('Predicted'), ax.set_ylabel(r'Actual')\n",
    "pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To contrast the previous graph, we will also plot our worst performing model (Random Forest Model). While the graph appears to still maintain lower errors at lower values (such as <500), it significantly deviates from the actual values at higher levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.asarray(dtModel\n",
    "                         .predict(test_data.map(lambda x: x.features))\n",
    "                         .collect())\n",
    "\n",
    "actual = np.asarray(test_data\n",
    "                    .map(lambda lp: lp.label).collect())\n",
    "\n",
    "error = np.asarray(pow(np.array(actual) - np.array(predictions), 2))\n",
    "\n",
    "norm = Normalize()\n",
    "clrs = cmap(np.asarray(norm(error)))[:,0:3]\n",
    "\n",
    "fig, ax = preparePlot(np.arange(0, 5000, 500), np.arange(0, 5000, 500))\n",
    "ax.set_xlim(0, 5000), ax.set_ylim(20, 5000)\n",
    "plt.scatter(predictions, actual, s=14**2, c=clrs, edgecolors='#888888', alpha=0.75, linewidths=.5)\n",
    "ax.set_xlabel('Predicted'), ax.set_ylabel(r'Actual')\n",
    "pass"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "DA3dJCFnponl"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
