{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7jb5rVb7k4W4"
   },
   "source": [
    "# Setup Spark in Google Colab\n",
    "*reference: https://www.analyticsvidhya.com/blog/2020/11/a-must-read-guide-on-how-to-work-with-pyspark-on-google-colab-for-data-scientists/*\n",
    "\n",
    "\n",
    "*to install other versions, get the download link from https://spark.apache.org/downloads.html*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RDm2hk6afyXH"
   },
   "outputs": [],
   "source": [
    "# !apt-get install openjdk-8-jdk-headless -qq > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XPLjf-tQgJ2B",
    "outputId": "fb241de8-cd57-4be0-b718-e7ca730a6c86"
   },
   "outputs": [],
   "source": [
    "# !wget https://dlcdn.apache.org/spark/spark-3.3.1/spark-3.3.1-bin-hadoop3.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nHDWo_1Kh9zj",
    "outputId": "ab76752d-15c1-4fba-fce0-c3c3e2297cbe"
   },
   "outputs": [],
   "source": [
    "# !tar -xvf spark-3.3.1-bin-hadoop3.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QF5q4lWYj_Hc",
    "outputId": "27e7dbeb-5c0c-4570-e20d-3f0c7ade5fe5"
   },
   "outputs": [],
   "source": [
    "# !pip install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M9_wQwdxkEp4"
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "# os.environ[\"SPARK_HOME\"] = \"/content/spark-3.3.1-bin-hadoop3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_GwoL0pbkf4f"
   },
   "outputs": [],
   "source": [
    "# import findspark\n",
    "# findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "sZu4rGOEkkOT",
    "outputId": "89f0720f-284b-4fb3-b7e6-9793e94cee7c"
   },
   "outputs": [],
   "source": [
    "# findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "To9reEGekk-T"
   },
   "outputs": [],
   "source": [
    "# from pyspark.sql import SparkSession\n",
    "\n",
    "# spark = SparkSession.builder\\\n",
    "#         .master(\"local\")\\\n",
    "#         .appName(\"Colab\")\\\n",
    "#         .config('spark.ui.port', '4050')\\\n",
    "#         .getOrCreate()\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#Create SparkSession\n",
    "spark = SparkSession.builder.master(\"local[12]\").appName(\"SparkTutorial\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "15-xw65skr_H"
   },
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RPwANqTjkwEO",
    "outputId": "d7d35d55-893f-49fe-8db8-1b21949e35b5"
   },
   "outputs": [],
   "source": [
    "test = sc.parallelize([1, 2, 3, 4, 5])\n",
    "test.map(lambda x: (x, x**2)).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0kvtPmuZKfru"
   },
   "source": [
    "# SENG 550 Project\n",
    "Sydney Kwok (30073206) \n",
    "Liam Conway (30046856)\n",
    "Isabella Guimet (30040654)\n",
    "Christina Truong (30064426)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DYYUbDXjjHtx"
   },
   "source": [
    "# Creating Our Dataset \n",
    "Fetch historical stock data for ten companies From Yahoo Finance using the yfinance library. Do a bit of processing to combine these all into one CSV that includes the Ticker as column data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "28e7YXdbj5qi",
    "outputId": "cf77d6ca-604b-4d1b-fd76-d5f7baad3c97"
   },
   "outputs": [],
   "source": [
    "!pip install yfinance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download all tickers for the S&P 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "be9oB12VjNaw",
    "outputId": "d33f40ad-c844-4818-e856-62c441d833ef"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def get_sp500():\n",
    "    # Get S&P 500 data from Wikipedia\n",
    "    data = pd.read_html('http://en.wikipedia.org/wiki/List_of_S%26P_500_companies')[0]\n",
    "    # Take only the selected columns\n",
    "    data = data[['Symbol', 'Security', 'GICS Sector']]\n",
    "    # Rename the selected columns\n",
    "    data = data.rename(columns={'Symbol': 'Ticker', 'Security': 'Company', 'GICS Sector': 'Sector'})\n",
    "    # Replace '.' with '-' for tickers (required for Yahoo Finance)\n",
    "    data['Ticker'] = data['Ticker'].str.replace('.', '-', regex=True)\n",
    "    # Remove commas from company names to prevent CSV misinterpretation\n",
    "    data['Company'] = data['Company'].str.replace(',', '', regex=True)\n",
    "    return data\n",
    "\n",
    "companies = get_sp500()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download all financial data for the S&P 500 tickers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "\n",
    "finance_data = yf.download(tickers=companies['Ticker'].tolist(), start='2010-01-01', end='2022-11-22', group_by='ticker')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge company data and financial data from the last two steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = []\n",
    "for index, company_data in companies.iterrows():\n",
    "    merged_data = finance_data[company_data['Ticker']].assign(**company_data)\n",
    "    dataframes.append(merged_data)\n",
    "\n",
    "final_df = pd.concat(dataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O52_wd-7aOXt",
    "outputId": "b72aa789-611d-40b0-c75e-bd59cc5030ab"
   },
   "outputs": [],
   "source": [
    "final_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 237
    },
    "id": "c63ZPsylTnlc",
    "outputId": "e55b81a9-410d-4a6a-8ac2-faa28fcddd40"
   },
   "outputs": [],
   "source": [
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 237
    },
    "id": "GLV3zKCbToNk",
    "outputId": "60be5313-ba6f-4428-eb23-78eae2eaec09"
   },
   "outputs": [],
   "source": [
    "final_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BDYPpsKKbWIc",
    "outputId": "5db2bffd-e7d6-4c11-c9b2-33e16c203545"
   },
   "outputs": [],
   "source": [
    "final_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wxbE1ragV2YX",
    "outputId": "927b6ad1-e025-4c40-a497-9ece5a2f879a"
   },
   "outputs": [],
   "source": [
    "final_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The null data consists of stock financial data that did not exist as of our given start date (January 1, 2010), as seen below. For instance, ZTS was listed on February 1, 2013."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_df[final_df.isnull().any(axis=1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As these stocks did not have any *public* value as of these dates, we'll fill all NaN values of 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = final_df.fillna(0)\n",
    "final_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZU3BSoB1jw9y"
   },
   "outputs": [],
   "source": [
    "final_df.to_csv('raw-data.csv', header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DA3dJCFnponl"
   },
   "source": [
    "# Load CSV Into PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RGnt_eoqup1a"
   },
   "source": [
    "Read the CSV and return it as an RDD of Strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mYT-twMpwonR"
   },
   "outputs": [],
   "source": [
    "rdd = sc.textFile('raw-data.csv', 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zmLxLHLyxVPT",
    "outputId": "5a8db782-ae27-4541-a82b-18433e459697"
   },
   "outputs": [],
   "source": [
    "rdd.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8KgBNvjPpuLs"
   },
   "source": [
    "# Exploratory Data Analysis\n",
    "Compute some stats on the historical stock data we've collected for the selected ten companies for the period January 1, 2010 to November 22, 2022."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SYtEsVe3dQSv"
   },
   "source": [
    "**1). Calculate the average close price for each ticker over the last 20 years**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fWUh-TbqfTsb"
   },
   "source": [
    "1a). Extract fields only relevant to this analysis. That is, we only need field 0 (Date), field 4 (Close) and field 7 (ticker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xdFScpknhLya",
    "outputId": "c0b25ccf-fc08-438e-ece8-d6d0cefd9971"
   },
   "outputs": [],
   "source": [
    "def extractFieldsForQ1(stockRDDRecord):\n",
    "  fieldsList = stockRDDRecord.split(\",\")\n",
    "  return (fieldsList[0], fieldsList[4], fieldsList[7])\n",
    "\n",
    "print(extractFieldsForQ1(rdd.take(1)[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "btH83OmriEek"
   },
   "source": [
    "Now, we apply this function \"extractFieldsForQ1()\" for all rows in our RDD using a map function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AmOF-WyViOf2",
    "outputId": "0d3e30d4-8bd4-46e9-d10e-de9046f224d6"
   },
   "outputs": [],
   "source": [
    "closeTickerRDD = rdd.map(extractFieldsForQ1)\n",
    "print(closeTickerRDD.take(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "closeTickerRDDFiltered = closeTickerRDD.filter(lambda x: float(x[1]) > 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BMF7QUgleAeB"
   },
   "source": [
    "1b). Calculate the number of close stock prices for each ticker. From above, we can see that the `closeTickerRDD` contains 3 values: ('date', 'close', 'ticker'), so we need to grab the 2nd index which is ticker and count how many rows we have for that ticker. We will save this value for later when we do the average calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r0tDwXATeXt6",
    "outputId": "5eeab2a2-47fe-4785-99b0-bb6e63149eff"
   },
   "outputs": [],
   "source": [
    "stockCountPerTickerRDD = (closeTickerRDDFiltered.map(lambda x : (x[2], 1))\n",
    "                            .reduceByKey(lambda x,y : x+y)\n",
    "                            .take(10)\n",
    "                      )\n",
    "print(stockCountPerTickerRDD)\n",
    "\n",
    "# Will help out later when calculating the averages\n",
    "stockCountPerTickerDict = dict(stockCountPerTickerRDD)\n",
    "print(stockCountPerTickerDict.get('TSLA'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UzxYsECopWo8"
   },
   "source": [
    "1c). Calculate the sum of all close stock prices for a particular stock for the last 20 years. Reminder that closeTickerRDD contains 3 values: ('date', 'close', 'ticker'). Ticker is `x[2]` and close is `x[1]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jFzXRHtkps0V",
    "outputId": "694d5c4a-f35f-49ad-f474-eea045a400c6"
   },
   "outputs": [],
   "source": [
    "sumClosePricePerTickerRDD = (closeTickerRDDFiltered.map(lambda x : (x[2], float(x[1])))\n",
    "                                .reduceByKey(lambda x,y: x+y)\n",
    "                                .take(10))\n",
    "print(sumClosePricePerTickerRDD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Fr81M02vA6M"
   },
   "source": [
    "1d) Calculate the average stock close price with `stockCountPerTickerRDD` and `avgClosePricePerTickerRDD`. \n",
    "\n",
    "Note:\n",
    "\n",
    "item[1] is the sum of close price per ticker\n",
    "\n",
    "stockCountPerTickerDict.get(item[0]) is the total number of stocks close prices for that ticker in `item[0]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O_J-2e2PpWOe",
    "outputId": "3811a7b7-b5f8-47c1-9df6-59c89080fa7a"
   },
   "outputs": [],
   "source": [
    "# This will hold the average stock close prices for each ticker\n",
    "avgStockClosePrices = list()\n",
    "\n",
    "for item in sumClosePricePerTickerRDD:\n",
    "  avg = item[1]/stockCountPerTickerDict.get(item[0])\n",
    "  avgStockClosePrices.append((item[0], avg))\n",
    "\n",
    "print(avgStockClosePrices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jju7CrazBI24"
   },
   "source": [
    "**2). Calculate the min \"Low\" value in this dataset for each ticker**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vtc01ULCB9PB",
    "outputId": "4ef1f555-fff6-434c-ff68-60f8d309dc6a"
   },
   "outputs": [],
   "source": [
    "# Just extract Ticker(7) & Low(3)\n",
    "def extractFieldsForQ2(stockRDDRecord):\n",
    "  fieldsList = stockRDDRecord.split(\",\")\n",
    "  return (fieldsList[7], float(fieldsList[3]))\n",
    "\n",
    "print(extractFieldsForQ2(rdd.take(1)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WrsqfxCsDKS-",
    "outputId": "1baa2f50-e95a-4dab-81ec-12b087f82b6c"
   },
   "outputs": [],
   "source": [
    "# Apply extractFieldsForQ2 on all rows in the RDD\n",
    "low_rdd = rdd.map(extractFieldsForQ2)\n",
    "print(low_rdd.take(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_rdd_filtered = low_rdd.filter(lambda x: float(x[1]) > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U0H94AwUENhP",
    "outputId": "fb605e44-fcc6-4ec3-a6c7-3b9f1b33a4b0"
   },
   "outputs": [],
   "source": [
    "# Calculate the min Low value for each ticker\n",
    "min_low_by_ticker = low_rdd_filtered.reduceByKey(lambda x,y: min(x,y)).take(10)\n",
    "\n",
    "for item in min_low_by_ticker:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ytS_xppvA6He"
   },
   "source": [
    "**3). Calculate the max \"High\" value in this dataset for each ticker**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1OkaKzt7CAkq",
    "outputId": "8b1c1780-7112-4038-cfd6-8008c5f18a89"
   },
   "outputs": [],
   "source": [
    "# Just extract Ticker(7) & High(2)\n",
    "def extractFieldsForQ3(stockRDDRecord):\n",
    "  fieldsList = stockRDDRecord.split(\",\")\n",
    "  return (fieldsList[7], float(fieldsList[2]))\n",
    "\n",
    "print(extractFieldsForQ3(rdd.take(1)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W2xCClQMDRdN",
    "outputId": "cc941894-1717-4fc0-c98d-79f32f12928a"
   },
   "outputs": [],
   "source": [
    "# Apply extractFieldsForQ3 on all rows in the RDD\n",
    "high_rdd = rdd.map(extractFieldsForQ3)\n",
    "print(high_rdd.take(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N2DKW9hdFbmW",
    "outputId": "3d69b172-c80b-4772-e2a8-5aed0f57bf8f"
   },
   "outputs": [],
   "source": [
    "# Calculate the max High value for each ticker\n",
    "max_high_by_ticker = high_rdd.reduceByKey(lambda x,y: max(x,y)).take(10)\n",
    "\n",
    "for item in max_high_by_ticker:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m7GhwJcxSwJB"
   },
   "source": [
    "**4. Graph the max \"High\" value in this dataset for each ticker per year**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0aDe8gqLULE4"
   },
   "source": [
    "4a). Extract fields only relevant to this analysis. That is, we only need field 0 (Date), 2 (High), and 7 (Ticker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ndRKikYtTNrb",
    "outputId": "6f205f8c-9e23-4c4e-f538-6014c6a431f2"
   },
   "outputs": [],
   "source": [
    "def extractFieldsForQ4(stockRDDRecord):\n",
    "  fieldsList = stockRDDRecord.split(\",\")\n",
    "  return (fieldsList[0], fieldsList[2], fieldsList[7]) # Extract Date (0), High (2), Ticker (7)\n",
    "\n",
    "print(extractFieldsForQ4(rdd.take(1)[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qKlIcFH3US8o"
   },
   "source": [
    "Now, we apply this function \"extractFieldsForQ4()\" for all rows in our RDD using a map function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3c9ubkWmTQe1",
    "outputId": "9d587994-7a7a-49cf-80d5-2b7ee156a172"
   },
   "outputs": [],
   "source": [
    "graphHighRDD = rdd.map(extractFieldsForQ4)\n",
    "print(graphHighRDD.take(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ReNcDIVUff_"
   },
   "source": [
    "4b). Calculate the high price of each year for each ticker. From above, we can see that the `graphHighRDD` contains 3 values: `('date', 'high', 'ticker')`. In order to parse the data properly, we'll need to extract only the year out of the date, group all the data by the ticker, group the highs by year, and then take the max of the highs. An example array element may look like the following:\n",
    "```python\n",
    "('AAAA', {2000: 1, 2001: 2})\n",
    "```\n",
    "\n",
    "We will save this value for later when drawing the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rlYD6PTDTU2G",
    "outputId": "ceca30f8-eef6-4fe8-c1c9-8f4a1dbd13cc"
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Function to group the highs by year, and place the maximum high into a dict\n",
    "def merge(x):\n",
    "    data = list(map(lambda y : (y[1], y[2]), x[1]))\n",
    "    years = {y[0] for y in data}\n",
    "    result = defaultdict(int)\n",
    "    for d in data:\n",
    "        year, value = int(d[0]), float(d[1])\n",
    "        if(value > result[year]):\n",
    "            result[year] = value\n",
    "\n",
    "    return x[0], dict(result)\n",
    "\n",
    "# Convert the data into the \n",
    "graphHighPerYearRDD = (graphHighRDD.map(lambda x : (x[2], x[0].split('-')[0], x[1]))\n",
    "                            .groupBy(lambda x : x[0])\n",
    "                            .map(merge))\n",
    "\n",
    "print(graphHighPerYearRDD.take(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qVDqhfTvVLJo"
   },
   "source": [
    "4c) Draw the graph from the `graphHighPerYearRDD` dataset above, using matplotlib.\n",
    "\n",
    "As some stocks may not have been around for the entire duration, we'll need to determine every unique year contained within all the stocks. If a stock does not contain data for a year, we'll assume the price is 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "e-kSVtZVTxPR",
    "outputId": "e348867c-1e15-42a2-849c-8ee76f7d9947"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "graphHighValues = graphHighPerYearRDD.take(10)\n",
    "\n",
    "# Determine every unique year contained within the graphHighPerYearRDD\n",
    "x_values = graphHighPerYearRDD.flatMap(lambda x: list(x[1].keys())).distinct().collect()\n",
    "x_values.sort()\n",
    "\n",
    "# Display the output for each unique ticker\n",
    "for row in graphHighValues:\n",
    "    y_values = []\n",
    "    for key in x_values:\n",
    "        y_values.append(row[1][key] if key in row[1] else 0)\n",
    "    \n",
    "    plt.plot(x_values, y_values, label=row[0])\n",
    "\n",
    "# Stylize and display the graph\n",
    "plt.title(\"Yearly Highs by Ticker\")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"High\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "92YvE5MXpw6s"
   },
   "source": [
    "# Developing & Testing ML Models\n",
    "Following \"Spark Tutorial 2 - ML\", we complete a supervised learning pipeline, using the Stock Data dataset we looked at in our EDA phase of the project. Our goal is to train a linear regression model to predict Close values given the Date, Ticker, Open, High, Low, & Volume.\n",
    "\n",
    "## Read & Parse The Initial Dataset\n",
    "We have already previously completed the process of reading the stock data into an RDD, where each element of the RDD is a comma-separated string containing the Date, Open, High, Low, Close, Adj Close, Volume, and Ticker (representing the stock data for a given stock on a given date).\n",
    "\n",
    "Let's now use the `count` method to see how many data points are in the RDD & the `take` method to preview the first 3 data points in its current raw, comma-separated string format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CALGEdrcp0GS",
    "outputId": "262db0c7-5408-43db-9641-a03ef41db940"
   },
   "outputs": [],
   "source": [
    "num_points = rdd.count()\n",
    "print(f\"There are {num_points} data points in the RDD.\")\n",
    "\n",
    "first_3_points = rdd.take(3)\n",
    "print(first_3_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Po0sC_NB_-b"
   },
   "source": [
    "## RDD Pre-Processing\n",
    "Prior to training our regression model, we must first complete some pre-processing on the RDD. \n",
    "\n",
    "As mentioned previously, the RDD currently contains Date (field 0), Open (1), High (2), Low (3), Close (4), Adj Close (5), Volume (6), and Ticker (7) data, but our goal is to train a linear regression model to predict Close values given the Date, Ticker, Open, High, Low, & Volume. Hence, the Adj Close field (field 5) will not be used in our model, and can now be processed out of our RDD.\n",
    "\n",
    "While we're here, we will also be reformatting Dates from \"YYYY-MM-DD\" to \"YYYY\", \"MM\", \"DD\".\n",
    "\n",
    "Also, we will convert our categorical Ticker data into a numerical format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aLNzrc3uEysd",
    "outputId": "cd315e11-b461-4591-b0b2-11c2104af700"
   },
   "outputs": [],
   "source": [
    "# should look for a more scalable way to convert this categorical data to numerical format\n",
    "tickers = companies['Ticker'].values.tolist()\n",
    "ticker_classes = {k: v for v, k in enumerate(tickers)}\n",
    "\n",
    "def category_to_num(category):\n",
    "  \"\"\"\n",
    "  Given a Ticker name/category, return the corresponding numerical value.\n",
    "  \"\"\"\n",
    "  return str(ticker_classes[category])\n",
    "\n",
    "def process_record(rdd_record):\n",
    "  \"\"\"\n",
    "  Takes a record of the form 'YYYY-MM-DD, Open, High, Low, Close, Adj Close, Volume, Ticker' and returns a record of the form 'Ticker, YYYY, MM, DD, Open, High, Low, Volume, Close'\n",
    "  \"\"\"\n",
    "  date, open, high, low, close, _, volume, ticker, _, _ = rdd_record.split(',')\n",
    "\n",
    "  # I think feeding a date in the form \"YYYY-MM-DD\" is not as useful to our model as giving it \"YYYY\", \"MM\", \"DD\" separately. \n",
    "  # \"YYYY-MM-DD\" is too specific/unique whereas \"YYYY\", \"MM\", \"DD\" gives it more general info it can potentially learn patterns from. Thoughts?\n",
    "  year, month, day = date.split('-')\n",
    "\n",
    "  fields = [category_to_num(ticker), year, month, day, open, high, low, volume, close]\n",
    "  return ','.join(fields)\n",
    "\n",
    "processed_rdd = rdd.map(process_record)\n",
    "print(processed_rdd.take(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zH59eASWQAi5"
   },
   "source": [
    "Now, we can use MLlib `LabeledPoint` objects to label our records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PFLHop4DQQcF",
    "outputId": "90e6fa51-d820-4a34-9e17-d0e83c6e7716"
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "import numpy as np\n",
    "\n",
    "# sample data point: '0,2010,01,04,6.8125,6.83050012588501,6.6570000648498535,151998000,6.695000171661377'\n",
    "# we want the Close field to be the label (the last field), and the rest to be features\n",
    "\n",
    "def label_record(record):\n",
    "    \"\"\"\n",
    "    Converts a record from a comma-separated string of the form 'Ticker, YYYY, MM, DD, Open, High, Low, Volume, Close' \n",
    "    into a `LabeledPoint` where Close is the label and the rest of the fields are all features.\n",
    "    \"\"\"\n",
    "    record_elements = record.split(',')\n",
    "    return LabeledPoint(record_elements[-1], record_elements[:-1])\n",
    "\n",
    "labeled_points = processed_rdd.map(label_record)\n",
    "first_point = labeled_points.take(1)\n",
    "first_point_features = first_point[0].features \n",
    "first_point_label = first_point[0].label\n",
    "print(first_point_features, first_point_label)\n",
    "\n",
    "num_features = len(first_point_features)\n",
    "print(f\"There are {num_features} features.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Je3YQmx1ykt8"
   },
   "source": [
    "Now, in order to normalize our values, we're going to scale our features. The result of this is that all of our features will span a similar range.\n",
    "\n",
    "We will follow the same process as shown in the ML_Linear_Regression tutorial: `(featureValue - meanOfFeatureValues) / standardDeviationOfFeatureValues`. \"For a given feature, the mean and standard deviation of its feature values are calculated. Then, the mean is subtracted from each value and the result is then divided by the standard deviation.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mOjFkq7Xz07K",
    "outputId": "499e76ae-6f14-4f03-da9c-47ea9705f0b4"
   },
   "outputs": [],
   "source": [
    "def normalize_features(labeled_point):\n",
    "  \"\"\"\n",
    "  Normalize the features of the LabeledPoint object, labeled_point.\n",
    "  \"\"\"\n",
    "  normalized_features = list()\n",
    "  for i in range(0, len(labeled_point.features)):\n",
    "    feature = (labeled_point.features[i] - broadcast_mean.value[i]) / broadcast_stdev.value[i]\n",
    "    normalized_features.insert(i, feature)\n",
    "  return LabeledPoint(labeled_point.label, normalized_features)\n",
    "\n",
    "\n",
    "def get_normalized_rdd(non_normalized_rdd): \n",
    "    \"\"\"\n",
    "    Normalizes the features of the LabeldPoints contained in non_normalized_rdd.\n",
    "    \"\"\"\n",
    "    mean_list = list()\n",
    "    stdev_list = list()\n",
    "    num_features = len(non_normalized_rdd.take(1)[0].features)\n",
    "    for i in range(0, num_features):\n",
    "        feature_rdd = non_normalized_rdd.map(lambda lp: lp.features[i])\n",
    "        feature_mean = feature_rdd.mean()\n",
    "        feature_stdev = feature_rdd.stdev()\n",
    "        mean_list.insert(i, feature_mean)\n",
    "        stdev_list.insert(i, feature_stdev)\n",
    "    global broadcast_mean \n",
    "    broadcast_mean = sc.broadcast(mean_list)\n",
    "    global broadcast_stdev \n",
    "    broadcast_stdev = sc.broadcast(stdev_list)\n",
    "    return non_normalized_rdd.map(normalize_features)\n",
    "\n",
    "normalized_points = get_normalized_rdd(labeled_points)\n",
    "print(normalized_points.take(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rSDxi35P34dy"
   },
   "source": [
    "Now, we will split our data into training, validation, and testing sets using the `randomSplit` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1XWKIeoZ4MsO",
    "outputId": "2366a1f5-042c-441e-c3ce-10beb91e26e2"
   },
   "outputs": [],
   "source": [
    "weights = [.6, .2, .2] # 60/20/20 train/validation/test split\n",
    "seed = 42\n",
    "\n",
    "train_data, val_data, test_data = normalized_points.randomSplit(weights, seed)\n",
    "\n",
    "train_data.cache()\n",
    "val_data.cache()\n",
    "test_data.cache()\n",
    "\n",
    "num_train = train_data.count()\n",
    "num_val = val_data.count()\n",
    "num_test = test_data.count()\n",
    "\n",
    "print(f\"The original dataset has {normalized_points.count()} data points.\")\n",
    "print(f\"There are {num_train} training data points, {num_val} validation data points, and {num_test} data points, for a total of {num_train + num_val + num_test} data points.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pfmzO-Fa7rTO"
   },
   "source": [
    "## Creating & Evaluating A Baseline Model\n",
    "Creating a baseline model that we can use to evaluate our machine learning model later. We will look back and see if our model performs better or worse than this baseline model that uses a very simple technique to predict the stock Close values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lxmYaDBN8tIN"
   },
   "source": [
    "### Average Label\n",
    "This baseline model always makes the same prediction: the averahe label in the training set. This is a constant prediction value that is completely independent of the given data point. \n",
    "\n",
    "For this model, we will compute this value: the average Close price for the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x3damlke9zZg",
    "outputId": "5cf316e5-1329-44f5-cc5a-bdd72b1cffa2"
   },
   "outputs": [],
   "source": [
    "avg_close_price = (train_data.map(lambda s: s.label)).mean()\n",
    "print(f\"The average Close price for the training set is {avg_close_price}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_0K8IPb3-aMC"
   },
   "source": [
    "Now, we use Root Mean Squared Error (RMSE) to evaluate how well this baseline model performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HfEbfrUZ-jQV",
    "outputId": "c5eec842-4cc9-4c00-e1a9-375bc17b24c3"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def squared_error(label, prediction):\n",
    "    \"\"\"\n",
    "    Calculates the squared error for a single prediction.\n",
    "    \"\"\"\n",
    "    sqr_error = (label - prediction) * (label - prediction)\n",
    "    return sqr_error\n",
    "\n",
    "def calc_rmse(labels_and_preds):\n",
    "    \"\"\"\n",
    "    Calculates the RMSE for an `RDD` of (label, prediction) tuples.\n",
    "    \"\"\"\n",
    "    sqr_sum = labels_and_preds.map(lambda s: squared_error(s[0], s[1])).sum()\n",
    "    return math.sqrt(sqr_sum / labels_and_preds.count())\n",
    "\n",
    "labels_and_preds_train = train_data.map(lambda s: (s.label, avg_close_price))\n",
    "rmse_train_base = calc_rmse(labels_and_preds_train)\n",
    "print(f\"The RMSE of the baseline model on the train set is {rmse_train_base}.\")\n",
    "\n",
    "labels_and_preds_val = val_data.map(lambda s: (s.label, avg_close_price))\n",
    "rmse_val_base = calc_rmse(labels_and_preds_val)\n",
    "print(f\"The RMSE of the baseline model on the validation set is {rmse_val_base}.\")\n",
    "\n",
    "labels_and_preds_test = test_data.map(lambda s: (s.label, avg_close_price))\n",
    "rmse_test_base = calc_rmse(labels_and_preds_test)\n",
    "print(f\"The RMSE of the baseline model on the test set is {rmse_test_base}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cltxj6OuBHow"
   },
   "source": [
    "This baseline model performs very terribly with a train RMSE of 801, validation RMSE of 815, and test RMSE of 768.\n",
    "\n",
    "## Training Linear Regression Models Using MLlib\n",
    "Now, let's train some machine learning models and see if we can get better performance.\n",
    "### Linear Regression With SGD\n",
    "We're going to start by using LinearRegressionWithSGD to train a model with L2 regularization and an intercept. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b-JsdtHrDhcV",
    "outputId": "88cc21fb-74cb-45a3-8c21-fe45166ead46"
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import DenseVector\n",
    "from pyspark.mllib.regression import LinearRegressionWithSGD\n",
    "# Values to use when training the linear regression model\n",
    "numIters = 500  # iterations\n",
    "alpha = 1.0  # step - learning rate (rate at which you change your weights in each iteration)\n",
    "miniBatchFrac = 1.0  # miniBatchFraction\n",
    "reg = 1e-1  # regParam\n",
    "regType = 'l2'  # regType\n",
    "useIntercept = True  # intercept\n",
    "\n",
    "# train the model\n",
    "firstModel = LinearRegressionWithSGD.train(train_data, numIters, alpha, miniBatchFrac, initialWeights=None, regParam=reg, regType=regType, intercept=useIntercept)\n",
    "\n",
    "# weightsLR1 stores the model weights; interceptLR1 stores the model intercept\n",
    "weightsLR1 = firstModel.weights\n",
    "interceptLR1 = firstModel.intercept\n",
    "\n",
    "print(f\"Linear Regression Model 1 weights: {weightsLR1}, intercept: {interceptLR1}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pln5C8B7FJIF"
   },
   "source": [
    "Now, use the `LinearRegressionModel.predict()` method to make a prediction on a sample point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5tp-eJrkFPt2",
    "outputId": "3532e2bd-e3db-4bc4-8f8d-101f4c42af1c"
   },
   "outputs": [],
   "source": [
    "sample_point = train_data.take(1)[0]\n",
    "sample_prediction = firstModel.predict(sample_point.features)\n",
    "print(sample_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7N2PStShF_LM"
   },
   "source": [
    "Now, let's evaluate RMSE of this model on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jkM7HcApF9ze",
    "outputId": "26277573-980b-4a3b-ede7-6e2baf843c8d"
   },
   "outputs": [],
   "source": [
    "labels_and_preds = val_data.map(lambda lp: (lp.label, firstModel.predict(lp.features)))\n",
    "rmse_valLR1 = calc_rmse(labels_and_preds)\n",
    "\n",
    "print(f\"Compared to our baseline model which achieved a validation RMSE of {rmse_val_base}, our first Linear Regression Model achieved a validation RMSE of {rmse_valLR1}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "idV7ZJgDG8-m"
   },
   "source": [
    "While we do see great improvement in performance from our baseline model, there's still quite a bit of improvement to be made. We will try different models and hyperparameters to see if we can improve the performance.\n",
    "\n",
    "### Trying a 2nd Order Model\n",
    "Let's see if a 2nd order model will generate better predictions than our linear model did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rx07F77fHy-9",
    "outputId": "9a3a15f8-4b5f-495a-8b9c-61442de741bf"
   },
   "outputs": [],
   "source": [
    "def transformOrderTwo(lp):\n",
    "    \"\"\"Transforms the features in the LabeledPoint object lp into higher order features.\n",
    "\n",
    "    Args:\n",
    "        lp - LabeledPoint object \n",
    "\n",
    "    Returns:\n",
    "        LabeledPoint: The object contains the label and the higher order features\n",
    "    \"\"\"\n",
    "    numAccSq = lp.features[0]*lp.features[0] # square the features\n",
    "    snoOnGrndSq = lp.features[1]*lp.features[1]\n",
    "    numAccSnoOnGrnd = lp.features[0]*lp.features[1]\n",
    "    return LabeledPoint(lp.label,[lp.features[0],lp.features[1],numAccSq,snoOnGrndSq,numAccSnoOnGrnd])\n",
    "\n",
    "orderTwoParsedSamplePoints = labeled_points.map(transformOrderTwo)\n",
    "print(orderTwoParsedSamplePoints.take(2))\n",
    "normalizedOrderTwoSamplePoints = get_normalized_rdd(orderTwoParsedSamplePoints)\n",
    "print(normalizedOrderTwoSamplePoints.take(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WX2sti3RIj6V"
   },
   "source": [
    "Creating training, validation, and testing sets for 2nd order model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K_WlsThtIs5-",
    "outputId": "ca92d731-a382-4ecb-f233-88305c7ee7c2"
   },
   "outputs": [],
   "source": [
    "weights = [.6, .2, .2] # 60/20/20 train/validation/test split\n",
    "seed = 42\n",
    "\n",
    "train_data_order2, val_data_order2, test_data_order2 = normalizedOrderTwoSamplePoints.randomSplit(weights, seed)\n",
    "\n",
    "num_train_o2 = train_data_order2.count()\n",
    "num_val_o2 = val_data_order2.count()\n",
    "num_test_o2 = test_data_order2.count()\n",
    "\n",
    "print(f\"The original dataset has {normalizedOrderTwoSamplePoints.count()} data points.\")\n",
    "print(f\"There are {num_train_o2} training data points, {num_val_o2} validation data points, and {num_test_o2} data points, for a total of {num_train_o2 + num_val_o2 + num_test_o2} data points.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QtXRFHCMJj6-"
   },
   "source": [
    "Train the 2nd order model on the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zT9GVwN_JncB",
    "outputId": "21b9f4f6-f4a0-4811-a022-2abd56183cc3"
   },
   "outputs": [],
   "source": [
    "secondModel = LinearRegressionWithSGD.train(train_data_order2, numIters, alpha, miniBatchFrac, initialWeights=None, regParam=reg, regType=regType, intercept=useIntercept)\n",
    "# weightsLR1 stores the model weights; interceptLR1 stores the model intercept\n",
    "weightsLR2 = secondModel.weights\n",
    "interceptLR2 = secondModel.intercept\n",
    "print(f\"2nd Order Model weights: {weightsLR2}, intercept: {interceptLR2}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CFu3AwfgJnxO"
   },
   "source": [
    "Evaluate RMSE of the 2nd order model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HB65fSXsJp1Z",
    "outputId": "ccc59ecb-2410-408d-9b53-78675a9a3c33"
   },
   "outputs": [],
   "source": [
    "labelsAndPredsOrderTwo = val_data_order2.map(lambda lp: (lp.label, secondModel.predict(lp.features)))\n",
    "rmseValLR2 = calc_rmse(labelsAndPredsOrderTwo)\n",
    "\n",
    "print(f\"Compared to our baseline model with a validation RMSE of {rmse_val_base} & our first Linear Regression Model with val RMSE of {rmse_valLR1}, our 2nd Order Model achieved val RMSE of {rmseValLR2}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wx4PASAnKuc8"
   },
   "source": [
    "Our 2nd order model performed much worse than our first Linear Regression model, and not much better than our baseline model.\n",
    "\n",
    "### Trying Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DzIG8ztRLBjh",
    "outputId": "3564103d-912c-4cdc-88ed-ac18b8dc0eb1"
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.tree import RandomForest\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "dtModel = RandomForest.trainRegressor(train_data, categoricalFeaturesInfo={},\n",
    "                                      numTrees=8, featureSubsetStrategy=\"auto\",\n",
    "                                      impurity='variance', maxDepth=5, maxBins=32)\n",
    "predictions = dtModel.predict(val_data.map(lambda x: x.features)).collect()\n",
    "labels = val_data.map(lambda x: x.label).collect()\n",
    "rmseValDT = np.sqrt(np.mean((np.array(predictions)-np.array(labels))**2))\n",
    "\n",
    "print(f\"Baseline model validation RMSE: {rmse_val_base}\")\n",
    "print(f\"First Linear Regression Model val RMSE: {rmse_valLR1}.\")\n",
    "print(f\"2nd Order Model val RMSE: {rmseValLR2}.\")\n",
    "print(f\"Random Forest Model val RMSE: {rmseValDT}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G5UIj4YyMAHl"
   },
   "source": [
    "Still, our best performing model is the initial Linear Regression Model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wpHCh9M0p09f"
   },
   "source": [
    "# Fine Tune & Evaluate Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q7jGijdvp3W5"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "DA3dJCFnponl"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
